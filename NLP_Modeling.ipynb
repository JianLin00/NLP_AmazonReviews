{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EU3lVLGi5zou",
      "metadata": {
        "id": "EU3lVLGi5zou"
      },
      "source": [
        "# WQF7007 Group 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1541c80f",
      "metadata": {
        "collapsed": true,
        "id": "1541c80f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "frN7h8NgTmxk",
      "metadata": {
        "id": "frN7h8NgTmxk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SRXhKkVB6KKe",
      "metadata": {
        "id": "SRXhKkVB6KKe"
      },
      "source": [
        "# Data Extraction and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BbUHJIEsG7T5",
      "metadata": {
        "id": "BbUHJIEsG7T5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"amazon_reviews.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PdxfLA2V6gLS",
      "metadata": {
        "id": "PdxfLA2V6gLS"
      },
      "source": [
        "## Initial Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "toLitNiA6ihX",
      "metadata": {
        "collapsed": true,
        "id": "toLitNiA6ihX"
      },
      "outputs": [],
      "source": [
        "# Check for null values and basic info\n",
        "df.info()\n",
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7J1FqJqj6xql",
      "metadata": {
        "id": "7J1FqJqj6xql"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing essential text fields\n",
        "df.dropna(subset=['reviewText'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eWNDLc5f7Hie",
      "metadata": {
        "id": "eWNDLc5f7Hie"
      },
      "source": [
        "## Sentiment Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a40bfa2",
      "metadata": {
        "id": "6a40bfa2"
      },
      "outputs": [],
      "source": [
        "# extract reviews and ratings\n",
        "reviews = df['reviewText'].tolist()\n",
        "ratings = df['overall'].tolist()\n",
        "\n",
        "# Create a new dataframe with reviews and ratings\n",
        "reviews_df = pd.DataFrame({\n",
        "    'review': reviews,\n",
        "    'rating': ratings\n",
        "})\n",
        "\n",
        "# Add a new column of sentiment based on ratings\n",
        "def sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 'positive'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "# Map sentiment labels to integers\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "\n",
        "\n",
        "reviews_df['sentiment'] = reviews_df['rating'].apply(sentiment)\n",
        "reviews_df['label'] = reviews_df['sentiment'].map(label_map)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ag0eVYuT7bd3",
      "metadata": {
        "id": "Ag0eVYuT7bd3"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zLQJzB_j7dvH",
      "metadata": {
        "id": "zLQJzB_j7dvH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)        # Remove HTML tags\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)       # Remove non-alphabetic characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "reviews_df['review'] = reviews_df['review'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df.to_csv(\"reviews.csv\")"
      ],
      "metadata": {
        "id": "GTfhKrny6f1F"
      },
      "id": "GTfhKrny6f1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dXCH2kqv72YV",
      "metadata": {
        "id": "dXCH2kqv72YV"
      },
      "source": [
        "## Measure Tokenization Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6463552",
      "metadata": {
        "id": "f6463552"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Check max length of reviews\n",
        "max_length = reviews_df['review'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True))).max()\n",
        "print(f'Max length of reviews: {max_length}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594cb865",
      "metadata": {
        "id": "594cb865"
      },
      "outputs": [],
      "source": [
        "# Filter reviews longer than max length\n",
        "reviews_df = reviews_df[reviews_df['review'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True))) <= 256]\n",
        "\n",
        "# reindex the dataframe\n",
        "reviews_df = reviews_df.reset_index(drop=True)\n",
        "display(reviews_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IEtChu5p_c4m",
      "metadata": {
        "id": "IEtChu5p_c4m"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa83711",
      "metadata": {
        "id": "ffa83711"
      },
      "outputs": [],
      "source": [
        "# Tokenize function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"review\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256  # or 256, 512 depending on your model/memory\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ia3oOkQW8PRK",
      "metadata": {
        "id": "Ia3oOkQW8PRK"
      },
      "source": [
        "## Stop Words Removal & Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z0beTiFz8TYM",
      "metadata": {
        "id": "z0beTiFz8TYM"
      },
      "outputs": [],
      "source": [
        "# Stop words removal and lemmatization not necessary for transformer models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e-nXpU_s9lWl",
      "metadata": {
        "id": "e-nXpU_s9lWl"
      },
      "source": [
        "## Conversion to HuggingFace dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e32a8c",
      "metadata": {
        "id": "98e32a8c"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "reviews_df = reviews_df[['review', 'label']]\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(reviews_df)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "display(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k3lubzwj_94q",
      "metadata": {
        "id": "k3lubzwj_94q"
      },
      "outputs": [],
      "source": [
        "print(tokenized_datasets.to_pandas().head()['review'])\n",
        "print(tokenized_datasets.to_pandas().head()['input_ids'])\n",
        "print(tokenized_datasets.to_pandas().head()['token_type_ids'])\n",
        "print(tokenized_datasets.to_pandas().head()['attention_mask'])\n",
        "print(tokenized_datasets.to_pandas().head()['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7HjQRtNjMKRd",
      "metadata": {
        "id": "7HjQRtNjMKRd"
      },
      "source": [
        "# Model training using DistilBert"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test spliting"
      ],
      "metadata": {
        "id": "tk31tg4FYbLl"
      },
      "id": "tk31tg4FYbLl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n8mhS5RO8oye",
      "metadata": {
        "id": "n8mhS5RO8oye"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Perform train-test split (20% test)\n",
        "dataset_dict = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": dataset_dict[\"train\"],\n",
        "    \"test\": dataset_dict[\"test\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TYGSGs9S9T5D",
      "metadata": {
        "id": "TYGSGs9S9T5D"
      },
      "source": [
        "## 1. Trainer only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dCD7aGZ_RP9",
      "metadata": {
        "id": "0dCD7aGZ_RP9"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# Load DistilBERT tokenizer and model for 3-class classification\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gK9MbNjw9f0b",
      "metadata": {
        "id": "gK9MbNjw9f0b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.asarray(preds)\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"neutral_f1\": report[\"1\"][\"f1-score\"],\n",
        "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
        "        \"recall\": report[\"macro avg\"][\"recall\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "ynmfU22IePFF"
      },
      "id": "ynmfU22IePFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MIndU21q-kGN",
      "metadata": {
        "id": "MIndU21q-kGN"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    seed = 42,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"neutral_f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p4WOEbsHcMQR",
      "metadata": {
        "id": "p4WOEbsHcMQR"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qoWAyTxFBIDS",
      "metadata": {
        "id": "qoWAyTxFBIDS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Run predictions using the Trainer\n",
        "predictions_output = trainer.predict(dataset_dict[\"test\"])\n",
        "\n",
        "# Get predicted labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
        "y_true = predictions_output.label_ids\n",
        "\n",
        "# Generate classification report\n",
        "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4v5MyDNI9gql",
      "metadata": {
        "id": "4v5MyDNI9gql"
      },
      "source": [
        "## 2. Weighted Loss with L1/L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedLossTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, l1_lambda=0.00000001, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "        self.l1_lambda = l1_lambda\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Use plain CrossEntropyLoss without weights\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        # Optional: Keep or skip L1 regularization\n",
        "        if self.l1_lambda > 0:\n",
        "            l1_reg = torch.tensor(0., device=labels.device)\n",
        "            for param in model.parameters():\n",
        "                l1_reg += torch.norm(param, 1)\n",
        "            loss += self.l1_lambda * l1_reg\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "vpI_c_5C8kkK"
      },
      "id": "vpI_c_5C8kkK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKaJhbwR-bEW",
      "metadata": {
        "id": "IKaJhbwR-bEW"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    seed = 42,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"neutral_f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.00000001,  # L2 regularization\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "# Initialize class weights tensor\n",
        "#class_weights_tensor = torch.tensor([1, 1, 1], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_4_hThLmqUWJ",
      "metadata": {
        "id": "_4_hThLmqUWJ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nNGOmTzHEHfa",
      "metadata": {
        "id": "nNGOmTzHEHfa"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics: epochs = 4 with L1/L2 regularization\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Run predictions using the Trainer\n",
        "predictions_output = trainer.predict(dataset_dict[\"test\"])\n",
        "\n",
        "# Get predicted labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
        "y_true = predictions_output.label_ids\n",
        "\n",
        "# Generate classification report\n",
        "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "OHfkEK0rDy4c"
      },
      "id": "OHfkEK0rDy4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "UFMtZed3Bhdv",
      "metadata": {
        "id": "UFMtZed3Bhdv"
      },
      "source": [
        "## 3. Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class PlainFocalTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Use FocalLoss without weights\n",
        "        loss_fct = FocalLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "elUxVSN7AxQk"
      },
      "id": "elUxVSN7AxQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zRUEMUNYCMt7",
      "metadata": {
        "id": "zRUEMUNYCMt7"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    seed = 42,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"neutral_f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = PlainFocalTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "H3CWK3VGA6Cj"
      },
      "id": "H3CWK3VGA6Cj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "liRUp57nEpyP"
      },
      "id": "liRUp57nEpyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics: epochs = 4 and PlainFocalLoss\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Run predictions using the Trainer\n",
        "predictions_output = trainer.predict(dataset_dict[\"test\"])\n",
        "\n",
        "# Get predicted labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
        "y_true = predictions_output.label_ids\n",
        "\n",
        "# Generate classification report\n",
        "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "z29VyC94Ck7a"
      },
      "id": "z29VyC94Ck7a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "77EmSgzY_iMm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}